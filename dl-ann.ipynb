{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_list = !pip list\n",
    "packages = [\"dabl\", \"imblearn\", \"keras-tuner\", \"mysql-connector-python\", \"numpy\", \"pandas\", \"sklearn\", \"tensorflow\"]\n",
    "for package in packages:\n",
    "    if not pip_list.grep(package):\n",
    "        !pip3 install {package}\n",
    "\n",
    "print(\"Package installations completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from pickle import dump\n",
    "\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.stats as stat\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from dabl import SimpleClassifier, SimpleRegressor, clean, plot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from IPython.display import display\n",
    "from keras_tuner.tuners import BayesianOptimization, Hyperband, RandomSearch\n",
    "from mysql.connector import errorcode\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.datasets import (load_breast_cancer, load_diabetes, load_iris,\n",
    "                              load_wine)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import (SelectPercentile, VarianceThreshold,\n",
    "                                       chi2, f_classif)\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import (RandomizedSearchCV, StratifiedKFold,\n",
    "                                     train_test_split)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import (MinMaxScaler, OneHotEncoder, OrdinalEncoder,\n",
    "                                   StandardScaler)\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "%matplotlib inline\n",
    "set_config(display=\"diagram\", print_changed_only=False)\n",
    "pd.set_option(\"display.float_format\", lambda f: \"%.2f\" % f)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "plt.rcParams[\"figure.figsize\"] = [18, 7]\n",
    "plt.style.use(\"dark_background\")\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "print(tf.test.gpu_device_name())\n",
    "print(os.system(\"cat /proc/cpuinfo | grep 'model name'\"))\n",
    "print(os.system(\"cat /proc/meminfo | grep 'MemTotal'\"))\n",
    "print(os.system(\"nvidia-smi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     conn = mysql.connector.connect(\n",
    "#         user=\"operations\",\n",
    "#         password=os.environ[\"OPENAI_API_KEY\"],\n",
    "#         host=\"vickk73mysqlserver.mysql.database.azure.com\",\n",
    "#         port=3306,\n",
    "#         database=\"\",\n",
    "#         ssl_ca=\"{ca-cert filename}\",\n",
    "#         ssl_disabled=False,\n",
    "#     )\n",
    "#     print(\"Connection established\")\n",
    "# except mysql.connector.Error as e:\n",
    "#     if e.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "#         print(\"Something is wrong with the user name or password\")\n",
    "#     elif e.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "#         print(\"Database does not exist\")\n",
    "#     else:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     tables = {table: pd.read_sql(f\"\"\"SELECT * FROM {table};\"\"\", conn) for table in pd.read_sql(\"SHOW TABLES\", conn).iloc[:, 0]}\n",
    "#     for table_name, table_dataframe in tables.items():\n",
    "#         print(f\"-------------------------------------------------------\\nTable: {table_name}\")\n",
    "#         display(table_dataframe)\n",
    "#     conn.close()\n",
    "#     print(f\"Discovered table(s): {len(tables)}.\\nDB Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table_name, table_dataframe in tables.items():\n",
    "#     print(f\"-------------------------------------------------------\\nTable: {table_name}\\nColumns: {table_dataframe.columns}\\nShape: {table_dataframe.shape}\")\n",
    "# df.rename({\"\": \"\"}, axis=1, inplace=True)\n",
    "# df = pd.merge(tables[\"\"], tables[\"\"], on=\"\")\n",
    "# df = pd.merge(df, tables[\"\"], on=\"\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_breast_cancer(as_frame=True)[\"frame\"]\n",
    "df = load_iris(as_frame=True)[\"frame\"]\n",
    "df = load_wine(as_frame=True)[\"frame\"]\n",
    "df = load_diabetes(as_frame=True)[\"frame\"]\n",
    "url = \"https://raw.githubusercontent.com/lyoh001/AzureML/main/data.csv\"\n",
    "df = pd.read_csv(url, delimiter=\",\", encoding=chardet.detect(requests.get(url).content)[\"encoding\"], thousands=\",\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "SEARCH = [\"hyperband\", \"random\", \"bayesian\"][0]\n",
    "EPOCHS = 500\n",
    "MAX_TRIALS = 20\n",
    "DUPLICATES = 0\n",
    "SCALER = 1\n",
    "CLASSIFICATION = 0\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"Current Shape: {df.shape}.\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"Duplicates Percentage: {df.duplicated().sum() / df.shape[0] * 100:.2f}%\")\n",
    "if DUPLICATES:\n",
    "    print(f\"Duplicates have been kept {df.shape}.\")\n",
    "else:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Duplicates have been removed {df.shape}.\")\n",
    "display(df.sample(3))\n",
    "y_label = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[y_label] = df[y_label].map({k: i for i, k in enumerate(df[y_label].unique(), 0)})\n",
    "df[y_label].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), cmap=\"Blues\", fmt=\".2f\", annot=True, linewidths=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()[y_label].sort_values().drop(y_label)\n",
    "sns.barplot(x=corr_matrix.values, y=corr_matrix.index).set_title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "corr_matrix = df.corr()[y_label].abs().sort_values(ascending=False).drop(y_label)\n",
    "sns.barplot(x=corr_matrix.values, y=corr_matrix.index, ax=ax[0]).set_title(\"Horizontal Correlation Matrix\")\n",
    "sns.barplot(x=corr_matrix.index, y=corr_matrix.values, ax=ax[1]).set_title(\"Vertical Correlation Matrix\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(X, threshold):\n",
    "    col_corr = set()\n",
    "    df_corr = X.corr().abs()\n",
    "    for i, _ in enumerate(df_corr.columns):\n",
    "        for j in range(i):\n",
    "            if (df_corr.iloc[i, j] >= threshold) and (\n",
    "                df_corr.columns[j] not in col_corr\n",
    "            ):\n",
    "                col_corr.add(df_corr.columns[i])\n",
    "    return col_corr\n",
    "\n",
    "col_drop = correlation(df.drop(y_label, axis=1), 0.85)\n",
    "df.drop(col_drop, inplace=True, axis=1)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"Current Shape: {df.shape}.\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"Highly correlated cols have been removed: {len(col_drop)}.\")\n",
    "print(f\"Highly correlated cols: {col_drop}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"Missing Values\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data=df.isnull().melt(value_name=\"missing\"),\n",
    "    y=\"variable\",\n",
    "    hue=\"missing\",\n",
    "    multiple=\"fill\",\n",
    "    height=9.25,\n",
    ")\n",
    "plt.axvline(0.3, color=\"white\")\n",
    "plt.title(\"Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%a: Weekday, abbreviated: Mon, Tues, Sat\n",
    "%A: Weekday, full name: Monday, Tuesday, Saturday\n",
    "%w: Weekday, decimal. 0=Sunday: 1, 2, 6\n",
    "%d: Day of month, zero-padded: 01, 02, 21\n",
    "%b: Month, abbreviated: Jan, Feb, Sep\n",
    "%B: Month, full name: January, February, September\n",
    "%m: Month number, zero-padded: 01, 02, 09\n",
    "%y: Year, without century, zero-padded: 02, 95, 99\n",
    "%Y: Year, with century: 1990, 2020\n",
    "%H: Hour (24 hour), zero padded: 01, 22\n",
    "%I: Hour (12 hour) zero padded: 01, 12\n",
    "%p: AM or PM: AM, PM\n",
    "%M: Minute, zero-padded: 01, 02, 43\n",
    "%S: Second, zero padded: 01, 32, 59\n",
    "%f: Microsecond, zero-padded: 000001, 000342, 999999\n",
    "%z: UTC offset ±HHMM[SS[.ffffff]]: +0000, -1030, -3423.234\n",
    "%Z: Time zone name: ITC, EST, CST\n",
    "%j: Day of year, zero-padded: 001, 365, 023\n",
    "%U: Week # of year, zero-padded. Sunday first day of week: 00, 01, 51\n",
    "%W: Week # of year, zero-padded. Monday first day of week: 00, 02, 51\n",
    "%c: Appropriate date and time: Monday Feb 01 21:30:00 1990\n",
    "%x: Appropriate Date: 02/01/90\n",
    "%X: Appropriate Time: 21:22:00\n",
    "\"\"\"\n",
    "# df[\"date\"] = pd.to_datetime(df[\"\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# df[\"year\"] = df[\"date\"].dt.year\n",
    "# df[\"month\"] = df[\"date\"].dt.month\n",
    "# df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "# df[\"day\"] = df[\"date\"].dt.days\n",
    "# df[\"day\"] = (df[\"\"] - df[\"\"]).dt.days\n",
    "# df[\"hours\"] = (df[\"\"] - df[\"\"]).dt.total_seconds() / 60\n",
    "# df[\"end date\"] = df[\"date\"].map(lambda d: datetime.datetime.strptime(f\"{d.year}-{d.month}-{calendar.monthrange(d.year, d.month)[-1]}\", \"%Y-%m-%d\"))\n",
    "# def isfloat(n):\n",
    "#     try:\n",
    "#         float(n)\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "# df[df[\"\"].map(lambda n: not isfloat(n))]\n",
    "# df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "# df = pd.merge(df1, df2, on=\"\", how=\"outer\")\n",
    "# df.rename({\"\": \"\"}, axis=1, inplace=True)\n",
    "# df.replace({\"\": 0, \"\": 1, \"unknown\": np.nan}, inplace=True)\n",
    "# df[\"\"] = df[\"\"].map(lambda x: {\"\": 0, \"\": 1}.get(x, np.nan))\n",
    "# df[\"\"] = df[\"\"].map(pd.to_numeric, errors=\"coerce\")\n",
    "# df[\"\"] = df[\"\"].astype(float)\n",
    "# df[\"\"] = df[\"\"].astype(str).str.lower()\n",
    "# df[\"\"] = df[\"\"].astype(str).str.replace(\"\", \"\")\n",
    "# df[\"\"] = df[\"\"].astype(str).str.split().str.get(0)\n",
    "# df[\"\"] = df[\"\"].astype(str).str.strip()\n",
    "# df[[\"\", \"\"]] = df[\"\"].astype(str).str.split(\" \", expand=True)\n",
    "# df.drop(df[df[\"\"] < 0].index, inplace=True)\n",
    "# df.drop([\"\"], inplace=True, axis=1)\n",
    "# df.dropna(subset=[y_label], inplace=True)\n",
    "# df[].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cat_oe = []\n",
    "preprocessor_cat_oe = make_pipeline(\n",
    "    (SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (OrdinalEncoder(categories=[[\"\", \"\"]])),\n",
    ")\n",
    "col_cat = [col for col in df.columns if np.issubsctype(df[col].dtype, np.object0) and col != y_label]\n",
    "col_num = [col for col in df.columns if np.issubsctype(df[col].dtype, np.number) and col != y_label]\n",
    "col_cat_ohe = [col for col in col_cat if col not in col_cat_oe]\n",
    "col_num_disc = [col for col in col_num if df[col].nunique() < 25]\n",
    "col_num_cont = [col for col in col_num if col not in col_num_disc]\n",
    "\n",
    "df_info = pd.DataFrame(\n",
    "    {\n",
    "        \"column\": [col for col in df.columns],\n",
    "        \"dtype\": [f\"{df[col].dtype}\" for col in df.columns],\n",
    "        \"na\": [f\"{df[col].isna().sum()}\" for col in df.columns],\n",
    "        \"na %\": [f\"{round(df[col].isna().sum() / df[col].shape[0] * 100)}%\" for col in df.columns],\n",
    "        \"outliers\": [f\"{((df[col] < (df[col].quantile(0.25) - 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25)))) | (df[col] > (df[col].quantile(0.75) + 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))))).sum()}\" if col in col_num else \"n/a\" for col in df.columns],\n",
    "        \"outliers %\": [f\"{round((((df[col] < (df[col].quantile(0.25) - 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25)))) | (df[col] > (df[col].quantile(0.75) + 1.5 * (df[col].quantile(0.75) - df[col].quantile(0.25))))).sum()) / df[col].shape[0] * 100)}%\" if col in col_num else \"n/a\" for col in df.columns],\n",
    "        \"kurtosis\": [f\"{df[col].kurtosis(axis=0, skipna=True):.2f}\" if col in col_num else \"n/a\" for col in df.columns],\n",
    "        \"skewness\": [f\"{df[col].skew(axis=0, skipna=True):.2f}\" if col in col_num else \"n/a\" for col in df.columns],\n",
    "        \"corr\": [f\"{round(df[col].corr(other=df[y_label]) * 100)}%\" if col in col_num else \"n/a\" for col in df.columns],\n",
    "        \"nunique\": [f\"{df[col].nunique()}\" for col in df.columns],\n",
    "        \"unique\": [sorted(df[col].unique()) if col in col_num else df[col].unique() for col in df.columns],\n",
    "    }\n",
    ").sort_values(by=\"dtype\", ascending=False)\n",
    "display(df_info)\n",
    "print(f\"Current Shape: {df.shape}.\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"total na %: {df.isnull().sum().sum() / np.product(df.shape) * 100:.2f}%\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"col_cat_oe ({len(col_cat_oe)}): {col_cat_oe}\")\n",
    "print(f\"col_cat_ohe ({len(col_cat_ohe)}): {col_cat_ohe}\")\n",
    "print(f\"col_num_disc ({len(col_num_disc)}): {col_num_disc}\")\n",
    "print(f\"col_num_cont ({len(col_num_cont)}): {col_num_cont}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"total cols for preprocessor: {len(col_cat_oe) + len(col_cat_ohe) + len(col_num_disc) + len(col_num_cont)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COLS = 100\n",
    "for col in col_cat_ohe:\n",
    "    indice = df[col].value_counts()[:MAX_COLS].index\n",
    "    df_temp = df[col].map(lambda value: value if value in indice else \"-\")\n",
    "    # df[col] = df_temp\n",
    "    print(df_temp.value_counts())\n",
    "    print(f\"unique values: {df_temp.nunique()}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "try:\n",
    "    display(df.describe(exclude=\"number\").T.style.background_gradient(cmap=\"Blues\"))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_cat_ohe:\n",
    "    sr_temp = df.groupby(col)[y_label].count() / df.shape[0]\n",
    "    df_temp = sr_temp[sr_temp > 0.01].index\n",
    "    df[col] = np.where(df[col].isin(df_temp), df[col], \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_cat:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    sns.countplot(x=df[col], ax=ax[0], hue=df[y_label] if CLASSIFICATION else None).set_xlabel(f\"{col}\")\n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45)\n",
    "    ax[1].pie(x=df[col].value_counts(), autopct=\"%.1f%%\", shadow=True, labels=df[col].value_counts().index)\n",
    "    ax[1].set_title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_num_disc + ([y_label] if CLASSIFICATION else []):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    sns.lineplot(x=df[col], y=df[y_label], ax=ax[0]).set_xlabel(f\"{col}\")\n",
    "    sns.distplot(x=df[col], ax=ax[1], rug=True).set_xlabel(f\"{col}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLIERS = [\"keep\", \"cap\", \"drop\", \"log\", \"log1p\", \"reciprocal\", \"sqrt\", \"exp\", \"boxcox\", \"boxcox1\"][0]\n",
    "col_outlier = [col for col in col_num_cont + ([] if CLASSIFICATION else [y_label]) if col in df.columns]\n",
    "q1, q3 = df[col_outlier].quantile(0.25), df[col_outlier].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_range, upper_range = q1 - (1.5 * iqr), q3 + (1.5 * iqr)\n",
    "condition = ~((df[col_outlier] < lower_range) | (df[col_outlier] > upper_range)).any(axis=1)\n",
    "print(f\"Current Shape: {df.shape}.\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"Scanning for outliers in {col_outlier}.\")\n",
    "print(f\"Outliers Percentage: {(df.shape[0] - df[condition].shape[0]) / df.shape[0] * 100:.2f}%\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"upper_range:\\n{upper_range}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"lower_range:\\n{lower_range}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "if OUTLIERS == \"keep\":\n",
    "    print(f\"Outliers have been kept {df.shape}.\")\n",
    "elif OUTLIERS == \"cap\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = np.where(df[col] < lower_range[col], lower_range[col], df[col])\n",
    "        df[col] = np.where(df[col] > upper_range[col], upper_range[col], df[col])\n",
    "    print(f\"Outliers have been capped {df.shape}.\")\n",
    "elif OUTLIERS == \"drop\":\n",
    "    df = df[condition]\n",
    "    print(f\"Outliers have been removed {df.shape}.\")\n",
    "elif OUTLIERS == \"log\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = np.log(df[col])\n",
    "    print(f\"Outliers have been log transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"log1p\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = np.log1p(df[col])\n",
    "    print(f\"Outliers have been log1p transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"reciprocal\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = (1 / df[col])\n",
    "    print(f\"Outliers have been reciprocal transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"sqrt\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = (df[col] ** 0.5)\n",
    "    print(f\"Outliers have been sqrt transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"exp\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = (df[col] ** (1/1.2))\n",
    "    print(f\"Outliers have been exp transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"boxcox\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = stat.boxcox(df[col])[0]\n",
    "    print(f\"Outliers have been boxcox transformed {df.shape}.\")\n",
    "elif OUTLIERS == \"boxcox1\":\n",
    "    for col in col_outlier:\n",
    "        df[col] = stat.boxcox(df[col] + 1)[0]\n",
    "    print(f\"Outliers have been boxcox1 transformed {df.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_num_cont + ([] if CLASSIFICATION else [y_label]):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "    sns.distplot(x=df[col], ax=ax[0], rug=True).set_xlabel(f\"{col}\")\n",
    "    sns.boxplot(x=df[col], ax=ax[1], notch=True).set_xlabel(f\"{col}\")\n",
    "    sm.qqplot(data=df[col], ax=ax[2], xlabel=col, ylabel=\"\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"{col}\\nSkew: {df[col].skew(axis=0, skipna=True):.2f}\\nKurtosis: {df[col].kurtosis(axis=0, skipna=True):.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_num_cont + ([] if CLASSIFICATION else [y_label]):\n",
    "# for col in []:\n",
    "    for trans in [\"keep\", \"log\", \"log1p\", \"reciprocal\", \"sqrt\", \"exp\", \"boxcox\", \"boxcox1\"]:\n",
    "        try:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "            if trans == \"keep\":\n",
    "                x = df[col]\n",
    "            elif trans == \"log\":\n",
    "                x = np.log(df[col])\n",
    "            elif trans == \"log1p\":\n",
    "                x = np.log1p(df[col])\n",
    "            elif trans == \"reciprocal\":\n",
    "                x=(1 / df[col])\n",
    "            elif trans == \"sqrt\":\n",
    "                x=(df[col] ** 0.5)\n",
    "            elif trans == \"exp\":\n",
    "                x=(df[col] ** (1/1.2))\n",
    "            elif trans == \"boxcox\":\n",
    "                x=pd.Series(stat.boxcox(df[col])[0], name=col)\n",
    "            else:\n",
    "                x=pd.Series(stat.boxcox(df[col] + 1)[0], name=col)\n",
    "            sns.distplot(x=x, ax=ax[0], rug=True).set_xlabel(f\"{col}\")\n",
    "            sns.boxplot(x=x, ax=ax[1], notch=True).set_xlabel(f\"{col}\")\n",
    "            sm.qqplot(data=x, ax=ax[2], xlabel=col, ylabel=\"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        finally:\n",
    "            for i in range(3):\n",
    "                ax[i].set_title(f\"{trans.title()} Transformation\")\n",
    "            print(\"-------------------------------------------------------\")\n",
    "            print(f\"{col} {trans.title()} Transformation\\nSkew: {x.skew(axis=0, skipna=True):.2f}\\nKurtosis: {x.kurtosis(axis=0, skipna=True):.2f}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Boxplots for Numeric Columns\")\n",
    "sns.boxplot(data=df[[col for col in col_num_cont]], orient=\"h\", notch=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe().T.style.background_gradient(cmap=\"Blues\").format(\"{:.2f}\"))\n",
    "display(df.quantile([0.01, 0.99]).T.style.background_gradient(cmap=\"Blues\").format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.groupby(col_cat + [y_label]).size().reset_index().rename({0: \"Count\"}, axis=1)\n",
    "display(df_plot)\n",
    "sns.barplot(x=y_label, y=\"Count\", hue=None, data=df_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in col_num:\n",
    "#     if CLASSIFICATION:\n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "#         sns.swarmplot(x=y_label, y=col, data=df, color=\"grey\", alpha=0.7, ax=ax)\n",
    "#         sns.boxenplot(x=y_label, y=col, data=df, ax=ax)\n",
    "#     else:\n",
    "#         sns.jointplot(x=col, y=y_label, hue=None, data=df, kind=\"reg\")\n",
    "#         sns.jointplot(x=col, y=y_label, hue=None, data=df, kind=\"hex\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df, hue=y_label if CLASSIFICATION else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df.drop(y_label, axis=1), df[y_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(y_label, axis=1), df[y_label]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=[None, y, y][CLASSIFICATION],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "preprocessor_cat_ohe = make_pipeline(\n",
    "    (SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse=False)),\n",
    ")\n",
    "preprocessor_num_disc = make_pipeline(\n",
    "    (KNNImputer(n_neighbors=5)),\n",
    ")\n",
    "preprocessor_num_cont = make_pipeline(\n",
    "    (KNNImputer(n_neighbors=5)),\n",
    ")\n",
    "preprocessor_col = make_column_transformer(\n",
    "    (preprocessor_cat_oe, col_cat_oe),\n",
    "    (preprocessor_cat_ohe, col_cat_ohe),\n",
    "    (preprocessor_num_disc, col_num_disc),\n",
    "    (preprocessor_num_cont, col_num_cont),\n",
    "    sparse_threshold=0\n",
    ")\n",
    "preprocessor = make_pipeline(\n",
    "    (preprocessor_col),\n",
    "    ([MinMaxScaler(), StandardScaler()][SCALER]),\n",
    "    (VarianceThreshold(threshold=0)),\n",
    "    (PCA())\n",
    ")\n",
    "X_train_processed, y_train_processed = preprocessor.fit_transform(X_train), y_train\n",
    "X_test_processed, y_test_processed = preprocessor.transform(X_test), y_test\n",
    "if CLASSIFICATION:\n",
    "    class_weights = {i: w for i, w in enumerate(class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_processed), y=y_train_processed))}\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "    sns.despine(left=True)\n",
    "    sns.countplot(y, ax=ax[0]).set_xlabel(\"y\")\n",
    "    sns.countplot(y_train, ax=ax[1]).set_xlabel(\"y_train\")\n",
    "    sns.countplot(y_train_processed, ax=ax[2]).set_xlabel(\"y_train_processed\")\n",
    "    plt.show()\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"class_weights: {class_weights}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"y:\\n{y.value_counts(normalize=True)}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"y_train_processed:\\n{y_train_processed.value_counts(normalize=True)}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(f\"y_test_processed:\\n{y_test_processed.value_counts(normalize=True)}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"col_cat_oe ({len(col_cat_oe)}): {col_cat_oe}\")\n",
    "print(f\"col_cat_ohe ({len(col_cat_ohe)}): {col_cat_ohe}\")\n",
    "print(f\"col_num_disc ({len(col_num_disc)}): {col_num_disc}\")\n",
    "print(f\"col_num_cont ({len(col_num_cont)}): {col_num_cont}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"total cols for preprocessor: {len(col_cat_oe) + len(col_cat_ohe) + len(col_num_disc) + len(col_num_cont)}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"X: {X.shape}\\tX_train: {X_train.shape}\\tX_train_processed:{X_train_processed.shape}\\tX_test: {X_test.shape}\\t\\tX_test_processed:{X_test_processed.shape}\")\n",
    "print(f\"y: {y.shape}\\ty_train: {y_train.shape}\\t\\ty_train_processed:{y_train_processed.shape}\\ty_test: {y_test.shape}\\t\\ty_test_processed:{y_test_processed.shape}\")\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_auto_model():\n",
    "    if CLASSIFICATION:\n",
    "        SimpleClassifier(random_state=RANDOM_STATE).fit(clean(df), target_col=y_label)\n",
    "    else:\n",
    "        SimpleRegressor(random_state=RANDOM_STATE).fit(clean(df), target_col=y_label)\n",
    "\n",
    "def build_ml_model():\n",
    "    tests = [\n",
    "        {\n",
    "            \"model\": make_pipeline_imb(\n",
    "                preprocessor_col,\n",
    "                [MinMaxScaler(), StandardScaler()][SCALER],\n",
    "                SMOTE(random_state=RANDOM_STATE),\n",
    "                VarianceThreshold(threshold=0),\n",
    "                PCA(),\n",
    "                SelectPercentile(),\n",
    "                RandomForestClassifier() if CLASSIFICATION else LinearRegression(),\n",
    "            )\n",
    "            if OVERSAMPLE\n",
    "            else make_pipeline_imb(\n",
    "                preprocessor_col,\n",
    "                [MinMaxScaler(), StandardScaler()][SCALER],\n",
    "                VarianceThreshold(threshold=0),\n",
    "                PCA(),\n",
    "                SelectPercentile(),\n",
    "                RandomForestClassifier() if CLASSIFICATION else LinearRegression(),\n",
    "            ),\n",
    "            \"params\": {\n",
    "                \"columntransformer__pipeline-3__knnimputer__n_neighbors\": [1, 3, 5, 7, 9],\n",
    "                \"columntransformer__pipeline-4__knnimputer__n_neighbors\": [1, 3, 5, 7, 9],\n",
    "                \"selectpercentile__percentile\": [i * 10 for i in range(1, 10)],\n",
    "                \"selectpercentile__score_func\": [chi2, f_classif],\n",
    "                \"randomforestclassifier__n_estimators\": [100, 150, 200, 500],\n",
    "                \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"],\n",
    "                \"randomforestclassifier__max_depth\": [5, 10, 20, 50, 100, 200],\n",
    "                \"randomforestclassifier__min_samples_split\": [2, 5, 10, 20, 50, 100, 200],\n",
    "                \"randomforestclassifier__min_samples_leaf\": [5, 10, 20, 50, 100, 200],\n",
    "                \"randomforestclassifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "            }\n",
    "            if CLASSIFICATION\n",
    "            else {\n",
    "                \"columntransformer__pipeline-3__knnimputer__n_neighbors\": [1, 3, 5, 7, 9],\n",
    "                \"columntransformer__pipeline-4__knnimputer__n_neighbors\": [1, 3, 5, 7, 9],\n",
    "                \"selectpercentile__percentile\": [i * 10 for i in range(1, 10)],\n",
    "                \"selectpercentile__score_func\": [chi2, f_classif],\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    for test in tests:\n",
    "        rscv = RandomizedSearchCV(\n",
    "            estimator=test[\"model\"],\n",
    "            param_distributions=test[\"params\"],\n",
    "            n_jobs=-1,\n",
    "            cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "            if CLASSIFICATION\n",
    "            else 10,\n",
    "            scoring=\"accuracy\" if CLASSIFICATION else \"r2\",\n",
    "            n_iter=10,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "        display(rscv)\n",
    "        rscv.fit(X_train, y_train)\n",
    "        print(\"===train============================\")\n",
    "        print(f\"{rscv.best_score_ * 100:.2f}%\\n{test['model'][-1]}\\n{rscv.best_params_}\")\n",
    "        print(\"===params============================\")\n",
    "        display(pd.DataFrame(rscv.cv_results_).sort_values(by=\"rank_test_score\"))\n",
    "        print(\"===test============================\")\n",
    "        print(f\"test score:{rscv.score(X_test, y_test) * 100:.2f}%\")\n",
    "        print(\"====end===========================\\n\")\n",
    "\n",
    "    build_auto_model()\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    if CLASSIFICATION:\n",
    "        print(\n",
    "            classification_report(\n",
    "                y_test,\n",
    "                rscv.predict(X_test),\n",
    "            )\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            tf.math.confusion_matrix(\n",
    "                y_test,\n",
    "                rscv.predict(X_test),\n",
    "            ),\n",
    "            cmap=\"Blues\",\n",
    "            fmt=\"d\",\n",
    "            annot=True,\n",
    "            linewidths=1,\n",
    "        )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Truth\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"r2: {r2_score(y_test, rscv.predict(X_test)):.3f} neg_mean_squared_error: -{mean_squared_error(y_test, rscv.predict(X_test)):_.3f}\"\n",
    "        )\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.regplot(y_train, y_train, color=\"darkorange\", label=\"Truth\")\n",
    "        sns.regplot(\n",
    "            y_test,\n",
    "            rscv.predict(X_test),\n",
    "            color=\"darkcyan\",\n",
    "            label=\"Predicted\",\n",
    "        )\n",
    "        plt.title(\n",
    "            \"Truth vs Predicted\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        plt.xlabel(\"Truth values\")\n",
    "        plt.ylabel(\"Predicted values\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.scatterplot(\n",
    "            data=pd.DataFrame(\n",
    "                {\n",
    "                    \"Predicted values\": rscv.predict(X_train),\n",
    "                    \"Residuals\": rscv.predict(X_train) - y_train,\n",
    "                }\n",
    "            ),\n",
    "            x=\"Predicted values\",\n",
    "            y=\"Residuals\",\n",
    "            color=\"darkorange\",\n",
    "            marker=\"o\",\n",
    "            s=35,\n",
    "            alpha=0.5,\n",
    "            label=\"Train data\",\n",
    "        )\n",
    "        sns.scatterplot(\n",
    "            data=pd.DataFrame(\n",
    "                {\n",
    "                    \"Predicted values\": rscv.predict(X_test),\n",
    "                    \"Residuals\": rscv.predict(X_test) - y_test,\n",
    "                }\n",
    "            ),\n",
    "            x=\"Predicted values\",\n",
    "            y=\"Residuals\",\n",
    "            color=\"darkcyan\",\n",
    "            marker=\"o\",\n",
    "            s=35,\n",
    "            alpha=0.7,\n",
    "            label=\"Test data\",\n",
    "        )\n",
    "        plt.title(\n",
    "            \"Predicted vs Residuals\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        plt.hlines(y=0, xmin=0, xmax=df[y_label].max(), lw=2, color=\"red\")\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.distplot((y_train - rscv.predict(X_train)))\n",
    "        plt.title(\"Error Terms\")\n",
    "        plt.xlabel(\"Errors\")\n",
    "        plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Truth\": y_test[:10].values,\n",
    "                \"Predicted\": rscv.predict(X_test[:10]).round(1),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def tune_dl_model(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int(\"input_00\", min_value=32, max_value=512, step=32),\n",
    "            input_shape=X_train_processed.shape[1:],\n",
    "        )\n",
    "    )\n",
    "    for i in range(1, hp.Int(\"num_layers\", min_value=2, max_value=64)):\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(f\"hidden_{i:02}\", min_value=32, max_value=512, step=32),\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dropout(hp.Float(\"dropout\", min_value=0, max_value=0.5, step=0.1)))\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=[1, 1, df[y_label].nunique()][CLASSIFICATION],\n",
    "            activation=[\"linear\", \"sigmoid\", \"softmax\"][CLASSIFICATION],\n",
    "        )\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=[\"mean_squared_error\", \"binary_crossentropy\", \"sparse_categorical_crossentropy\"][CLASSIFICATION],\n",
    "        metrics=[\"mean_squared_error\", \"accuracy\", \"accuracy\"][CLASSIFICATION],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_dl_model(epochs):\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    model.fit(\n",
    "        X_train_processed,\n",
    "        y_train_processed,\n",
    "        batch_size=256 if tf.config.list_physical_devices(\"GPU\") else 64,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        class_weight=class_weights if OVERSAMPLE else None\n",
    "    )\n",
    "    build_auto_model()\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    if CLASSIFICATION:\n",
    "        print(\n",
    "            classification_report(\n",
    "                y_test_processed,\n",
    "                [\n",
    "                    model.predict(X_test_processed).round(),\n",
    "                    np.argmax(model.predict(X_test_processed), axis=1),\n",
    "                ][CLASSIFICATION - 1],\n",
    "            )\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            tf.math.confusion_matrix(\n",
    "                y_test_processed,\n",
    "                [\n",
    "                    model.predict(X_test_processed).round(),\n",
    "                    np.argmax(model.predict(X_test_processed), axis=1),\n",
    "                ][CLASSIFICATION - 1],\n",
    "            ),\n",
    "            cmap=\"Blues\",\n",
    "            fmt=\"d\",\n",
    "            annot=True,\n",
    "            linewidths=1,\n",
    "        )\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Truth\")\n",
    "\n",
    "    else:\n",
    "        print(f\"r2: {r2_score(y_test_processed, model.predict(X_test_processed).T[0]):.3f} neg_mean_squared_error: -{mean_squared_error(y_test_processed, model.predict(X_test_processed)):_.3f}\")\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.regplot(y_train_processed, y_train_processed, color=\"darkorange\", label=\"Truth\")\n",
    "        sns.regplot(\n",
    "            y_test_processed,\n",
    "            model.predict(X_test_processed).T[0],\n",
    "            color=\"darkcyan\",\n",
    "            label=\"Predicted\",\n",
    "        )\n",
    "        plt.title(\n",
    "            \"Truth vs Predicted\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        plt.xlabel(\"Truth values\")\n",
    "        plt.ylabel(\"Predicted values\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.scatterplot(\n",
    "            data=pd.DataFrame(\n",
    "                {\n",
    "                    \"Predicted values\": model.predict(X_train_processed).T[0],\n",
    "                    \"Residuals\": model.predict(X_train_processed).T[0] - y_train_processed,\n",
    "                }\n",
    "            ),\n",
    "            x=\"Predicted values\",\n",
    "            y=\"Residuals\",\n",
    "            color=\"darkorange\",\n",
    "            marker=\"o\",\n",
    "            s=35,\n",
    "            alpha=0.5,\n",
    "            label=\"Train data\",\n",
    "        )\n",
    "        sns.scatterplot(\n",
    "            data=pd.DataFrame(\n",
    "                {\n",
    "                    \"Predicted values\": model.predict(X_test_processed).T[0],\n",
    "                    \"Residuals\": model.predict(X_test_processed).T[0] - y_test_processed,\n",
    "                }\n",
    "            ),\n",
    "            x=\"Predicted values\",\n",
    "            y=\"Residuals\",\n",
    "            color=\"darkcyan\",\n",
    "            marker=\"o\",\n",
    "            s=35,\n",
    "            alpha=0.7,\n",
    "            label=\"Test data\",\n",
    "        )\n",
    "        plt.title(\n",
    "            \"Predicted vs Residuals\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        plt.hlines(y=0, xmin=0, xmax=df[y_label].max(), lw=2, color=\"red\")\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.distplot((y_train_processed - model.predict(X_train_processed).T[0]))\n",
    "        plt.title(\"Error Terms\")\n",
    "        plt.xlabel(\"Errors\")\n",
    "        plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Truth\": y_test_processed[:10].values,\n",
    "                \"Predicted\": [\n",
    "                    model.predict(X_test_processed[:10]).T[0],\n",
    "                    model.predict(X_test_processed[:10]).T[0].round(),\n",
    "                    np.argmax(model.predict(X_test_processed[:10]), axis=1),\n",
    "                ][CLASSIFICATION],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if SEARCH == \"hyperband\":\n",
    "    tuner = Hyperband(\n",
    "        tune_dl_model,\n",
    "        objective=[\"val_mean_squared_error\", \"val_accuracy\", \"val_accuracy\"][CLASSIFICATION],\n",
    "        max_epochs=MAX_TRIALS,\n",
    "        factor=3,\n",
    "        directory=\".\",\n",
    "        project_name=\"keras_tuner\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "elif SEARCH == \"random\":\n",
    "    tuner = RandomSearch(\n",
    "        tune_dl_model,\n",
    "        objective=[\"val_mean_squared_error\", \"val_accuracy\", \"val_accuracy\"][CLASSIFICATION],\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=3,\n",
    "        directory=\".\",\n",
    "        project_name=\"keras_tuner\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "else:\n",
    "    tuner = BayesianOptimization(\n",
    "        tune_dl_model,\n",
    "        objective=[\"val_mean_squared_error\", \"val_accuracy\", \"val_accuracy\"][CLASSIFICATION],\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=3,\n",
    "        directory=\".\",\n",
    "        project_name=\"keras_tuner\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=int(MAX_TRIALS/4))\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_auto_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERSAMPLE = 0\n",
    "build_ml_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from pycaret.classification import *\n",
    "# from pycaret.regression import *\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/lyoh001/AzureML/main/data.csv\"\n",
    "# df = pd.read_csv(url, delimiter=\",\", encoding=chardet.detect(requests.get(url).content)[\"encoding\"], thousands=\",\")\n",
    "# y_label = \"target\"\n",
    "\n",
    "# model = setup(data=df, target=y_label)\n",
    "# compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# OVERSAMPLE = 0\n",
    "# tuner.search(\n",
    "#     X_train_processed,\n",
    "#     y_train_processed,\n",
    "#     batch_size=256 if tf.config.list_physical_devices(\"GPU\") else 64,\n",
    "#     callbacks=[early_stop],\n",
    "#     epochs=MAX_TRIALS,\n",
    "#     validation_split=0.2,\n",
    "#     verbose=1,\n",
    "# )\n",
    "# tuner.results_summary()\n",
    "\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# model = tuner.hypermodel.build(best_hps)\n",
    "# history = model.fit(\n",
    "#     X_train_processed,\n",
    "#     y_train_processed,\n",
    "#     batch_size=256 if tf.config.list_physical_devices(\"GPU\") else 64,\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_split=0.2,\n",
    "#     verbose=1,\n",
    "#     class_weight=class_weights if OVERSAMPLE else None\n",
    "# )\n",
    "# val_per_epoch = history.history[\n",
    "#     [\"val_mean_squared_error\", \"val_accuracy\", \"val_accuracy\"][CLASSIFICATION]\n",
    "# ]\n",
    "# best_epoch = val_per_epoch.index([min(val_per_epoch), max(val_per_epoch), max(val_per_epoch)][CLASSIFICATION]) + 1\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.lineplot(data=history.history[[\"mean_squared_error\", \"accuracy\", \"accuracy\"][CLASSIFICATION]], color=\"deeppink\", linewidth=2.5)\n",
    "# sns.lineplot(data=history.history[[\"val_mean_squared_error\", \"val_accuracy\", \"val_accuracy\"][CLASSIFICATION]], color=\"darkturquoise\", linewidth=2.5)\n",
    "# plt.title(\"Model Accuracy\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Training Accuracy\", \"Val Accuracy\"], loc=\"lower right\")\n",
    "# plt.grid()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.lineplot(data=history.history[\"loss\"], color=\"deeppink\", linewidth=2.5)\n",
    "# sns.lineplot(data=history.history[\"val_loss\"], color=\"darkturquoise\", linewidth=2.5)\n",
    "# plt.title(\"Model Loss\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.legend([\"Training Loss\", \"Val Loss\"], loc=\"upper right\")\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Best epoch: {best_epoch}\")\n",
    "# model = build_dl_model(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_dl_model(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f\"dl_model_{time_stamp}\")\n",
    "# shutil.make_archive(f\"dl_model_{time_stamp}\", \"zip\", f\"./dl_model_{time_stamp}\")\n",
    "# dump(preprocessor, open(f\"dl_preprocessor.pkl\", \"wb\"))\n",
    "# model.summary()\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import shutil\n",
    "# import warnings\n",
    "# from pickle import load\n",
    "\n",
    "# import azure.functions as func\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "#     logging.info(\"*******Starting main function*******\")\n",
    "#     logging.info(f\"Request query: {req.get_json()}\")\n",
    "#     shutil.unpack_archive(\"dl_model.zip\", \"dl_model\")\n",
    "#     model = keras.models.load_model(\"dl_model\")\n",
    "#     preprocessor = load(open(\"dl_preprocessor/dl_preprocessor.pkl\", \"rb\"))\n",
    "#     payload = pd.DataFrame(\n",
    "#         {k: [np.nan] if next(iter(v)) == \"\" else v for k, v in req.get_json().items()},\n",
    "#         dtype=\"object\",\n",
    "#     )\n",
    "#     logging.info(\"*******Finishing main function*******\")\n",
    "#     return func.HttpResponse(\n",
    "#         status_code=200,\n",
    "#         body=f\"{model.predict(preprocessor.transform(payload))[0][0]:.2f}\",\n",
    "#     )\n",
    "#     return func.HttpResponse(\n",
    "#         status_code=200,\n",
    "#         body=[\"\", \"\"][\n",
    "#             np.argmax(model.predict(preprocessor.transform(payload)))\n",
    "#         ],\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
